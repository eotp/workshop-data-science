{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial Regression: The Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the `src` directory as one where we can import modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir, 'src'))\n",
    "sys.path.append(src_dir)\n",
    "print(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The machine learning model development process\n",
    "\n",
    "<img src=\"./_img/ML_scheme.png\", style=\"height: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import helper_funcs as hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load `train_test_split`, `one_step_learning`, `predict` and `rmse` functions we defined in the previous notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ../src/modelling_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In this example the data is generated from a function that is unknown to you. At the end of this segment we uncover the secret of the data generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hf.data_generator(60, rs=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bunch of data points, each data point is a $(x,y)$ pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the data in form of a scatter plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x=\"x\", y=\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(data, ratio=0.2, rs=44) \n",
    "X_train = train.x.values.reshape(-1,1)\n",
    "y_train = train.y.values\n",
    "X_val = val.x.values.reshape(-1,1)\n",
    "y_val = val.y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "train.plot.scatter(x=\"x\", y=\"y\", ax=ax, \n",
    "                   alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "val.plot.scatter(x=\"x\", y=\"y\", ax=ax, \n",
    "                  alpha=0.5, s=60, color=\"g\", label=\"Test set\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Polynomial regression](https://en.wikipedia.org/wiki/Polynomial_regression) is a special type of linear regression in which the relationship between the predictor variable $x$ and the response variable $y$ is modeled by a k<sup>th</sup>-degree polynomial in $x$. The incorporation of k<sup>th</sup>-degree polynomials results in a nonlinear relation between $y$ and $x$, but between the parameters $(\\beta_i)$ and the expected observations is linear. The model equation can be written as \n",
    "\n",
    "$$\\hat y = \\beta_0+\\beta_1x+\\beta_2x^2+...+\\beta_kx^k+\\epsilon$$\n",
    "\n",
    "Finding the optimal parameter combination is done by minimizing the **sum of squared errors (SSE)**, given by the equation\n",
    "\n",
    "$$SSE = \\sum e^2 = \\sum (\\hat y - y)^2 $$\n",
    "\n",
    "By fitting a polynomial to observations there arises the problem of choosing the order $k$ of the polynomial. How to choose the right number for the polynomial is a matter of an important concept called **model comparison** or [**model selection**](https://en.wikipedia.org/wiki/Model_selection). To keep it simple we use the [**root-mean-square error  (RMSE)**](https://en.wikipedia.org/wiki/Root-mean-square_deviation) defined by\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{\\sum_{i=1}^n (\\hat y - y)^2}{n}}$$\n",
    "\n",
    "to evaluate the goodness-of-fit of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamter: Generate polynomial and interaction features (Feature engineering).\n",
    "\n",
    "\n",
    "$$\\text{e.g. 2nd order:} \\qquad (x,y) \\to (x,y,x^2, xy,y^2)$$ \n",
    "\n",
    "\n",
    "\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` library provides powerful functionality to create polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly2d = PolynomialFeatures(degree=2, \n",
    "                            interaction_only=False, \n",
    "                            include_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.array([[1,10], \n",
    "             [2,20],\n",
    "             [3,30]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly2d.fit_transform(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create polynomial features, we \n",
    "\n",
    "* build 6 different models, with $k = 1,2,3,5,9,14$. For each model we \n",
    "\n",
    "* calculate the RMSE on the training set and on the validation. \n",
    "\n",
    "* Finally, we plot the data together with the regression line, given by each particular model. For convenience we construct a loop to reduce the amount of coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_polynomial_model(X_train, y_train, X_val, y_val, degree):\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    \n",
    "    ### create polynomial features ###\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    poly.fit(X_train)\n",
    "    # polynomials for the train set\n",
    "    X_train_poly = poly.transform(X_train)\n",
    "    # polynomials for the validation set\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    # polynomials for the regression line\n",
    "    X_predict_reg_line = poly.transform(np.linspace(0,1, 100).reshape(-1,1))\n",
    "        \n",
    "    ### learn paramters ###\n",
    "    beta = one_step_learning(X_train_poly, y_train)\n",
    "        \n",
    "    ### create regression line ###\n",
    "    y_predict_reg_line = predict(X_predict_reg_line, beta)\n",
    "\n",
    "    ### predict on training set ###\n",
    "    y_train_predict = predict(X_train_poly, beta)\n",
    "    ## RMSE on training set\n",
    "    rmse_train = rmse(y_train_predict, y_train)\n",
    "\n",
    "    ### predict validation set ###\n",
    "    y_val_predict = predict(X_val_poly, beta)\n",
    "    ## RMSE on validation set\n",
    "    rmse_val = rmse(y_val_predict, y_val)\n",
    "\n",
    "    return rmse_train, rmse_val, y_predict_reg_line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_polynomial_model(train, val,\n",
    "                               x_regline, y_regline, \n",
    "                               rmse_train, rmse_val,\n",
    "                               degree, ax):\n",
    "    \n",
    "    train.plot.scatter(x=\"x\", y=\"y\", ax=ax, \n",
    "                   alpha=0.5, s=60, color=\"r\", label=\"Training set\")\n",
    "    val.plot.scatter(x=\"x\", y=\"y\", ax=ax, \n",
    "                      alpha=0.5, s=60, color=\"g\", label=\"Validation set\")\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.plot(x_regline, y_regline)\n",
    "    ax.text(s=r\"$RMSE_T:$ {}\".format(np.round(rmse_train,3)), \n",
    "            x=0.75, y=1.15, size=16)\n",
    "    ax.text(s=r\"$RMSE_V:$ {}\".format(np.round(rmse_val,3)), \n",
    "            x=0.75,y=0.85, size=16)\n",
    "    ax.set_ylim(-1.95,1.95)\n",
    "    ax.set_title(\"Polynomial Regression (degree: {})\".format(degree), \n",
    "                 size=18);    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rmse_train, \n",
    " rmse_val, \n",
    " y_predict_reg_line) = build_polynomial_model(X_train,\n",
    "                                              y_train, \n",
    "                                              X_val,\n",
    "                                              y_val,\n",
    "                                              degree=2)\n",
    "\n",
    "fix, ax = plt.subplots(figsize=(8,4))\n",
    "visualize_polynomial_model(train=train, val=val, \n",
    "                           x_regline=np.linspace(0,1, 100),\n",
    "                           y_regline=y_predict_reg_line,\n",
    "                           rmse_train=rmse_train, rmse_val=rmse_val,\n",
    "                           degree=2, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(4,2,figsize=(15,16))\n",
    "axes = axes.ravel()\n",
    "rmse_train_metric = []\n",
    "rmse_val_metric = []\n",
    "degrees = [1,2,3,5,7,9,10,12]\n",
    "for e, degree in enumerate(degrees):\n",
    "    (rmse_train, \n",
    "     rmse_val, \n",
    "     y_predict_reg_line) = build_polynomial_model(X_train,\n",
    "                                                  y_train,\n",
    "                                                  X_val,\n",
    "                                                  y_val,\n",
    "                                                  degree=degree)\n",
    "    rmse_train_metric.append(rmse_train)\n",
    "    rmse_val_metric.append(rmse_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    visualize_polynomial_model(train=train, val=val, \n",
    "                               x_regline=np.linspace(0,1, 100),\n",
    "                               y_regline=y_predict_reg_line,\n",
    "                               rmse_train=rmse_train, rmse_val=rmse_val,\n",
    "                               degree=degree, ax=axes[e])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, pretty plots! The figure shows, that if we increase $k$, the order of the polynomial, the curve becomes more flexible and it fits the data better and better. The better the data is fitted the lower becomes the error, RMSE. \n",
    "\n",
    "What is the best polynomial to fit the data?   \n",
    "\n",
    "Recall the goal is to learn the parameters from the data, thus we are interested in achieving a good generalization of the model and not necessarily perfectly fitted observation data. Such a behavior is known as [**overfitting**](https://en.wikipedia.org/wiki/Overfitting).\n",
    "\n",
    "\n",
    "For convenience we plot the RMSE against $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build data frame\n",
    "res = pd.DataFrame({\"RMSE Training\":rmse_train_metric,\n",
    "                    \"RMSE Validation\":rmse_val_metric},\n",
    "                    index=degrees)\n",
    "# compute sweet spot\n",
    "sweet_spot_y = res[\"RMSE Validation\"].min()\n",
    "sweet_spot_x = res[\"RMSE Validation\"].idxmin()\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "res.plot(ax=ax, linewidth=4)\n",
    "ax.scatter(x=sweet_spot_x, y=sweet_spot_y, s=200, \n",
    "           alpha=0.5, label=\"Sweet spot\", color=\"g\")\n",
    "ax.plot((sweet_spot_x, sweet_spot_x),(0,sweet_spot_y),\n",
    "        linestyle=\"dashed\", color=\"g\")\n",
    "ax.plot((sweet_spot_x, sweet_spot_y),(sweet_spot_y, sweet_spot_y),\n",
    "        linestyle=\"dashed\", color=\"g\")\n",
    "\n",
    "ax.set_ylim(0.2,1)\n",
    "ax.set_xlim(0.5,12)\n",
    "ax.set_xlabel(\"Degree\")\n",
    "ax.set_ylabel(\"RMSE\")\n",
    "ax.legend()\n",
    "ax.set_title(\"Variance-Bias Tradeoff\", size=22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure shows that the error on the training data (blue line) is constantly decreasing. If we take a look at the RMSE for the validation set (orange line), we see that with increasing $k$, and thus increasing model complexity, the error decreases.\n",
    "\n",
    "Note that there is a sweet spot, indicated by the lowest RMSE on the validation set, where the model is just complex enough to generalize well on the so far unseen validation data. In our example the sweet spot is obtained for a regression model of 5<sup>th</sup> order. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
